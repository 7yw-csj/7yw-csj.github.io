{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"Spark基础配置","slug":"one","date":"2022-05-22T03:11:00.000Z","updated":"2022-05-22T12:55:30.689Z","comments":true,"path":"2022/05/22/one/","link":"","permalink":"http://example.com/2022/05/22/one/","excerpt":"","text":"*一、配置基础环境* #主机名 1cat /etc/hostname # hosts映射 1vim /etc/hosts 1234567891011127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.151 node1.itcast.cn node1 192.168.88.152 node2.itcast.cn node2 192.168.88.153 node3.itcast.cn node3 *二、安装配置jdk* \\1. 编译环境软件安装目录 1mkdir -p /export/server \\2. 上传jdk-8u65-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录并解压 123rztar -zxvf jdk-8u65-linux-x64.tar.gz \\3. 配置环境变量 1vim /etc/profile 12345export JAVA_HOME=/export/server/jdk1.8.0_241export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar \\4. 重新加载环境变量文件 1source /etc/profile \\5. 查看java版本号 1Java -version \\6. 将java由node1分发到node2、node3 123scp -r /export/server/jdk1.8.0_241/ root@node2:/export/serverscp -r /export/server/jdk1.8.0_241/ root@node3:/export/server \\7. 配置node2、node3的环境变量文件（方法如上） \\8. 在node1、node2、node3中创建软连接（三台都需要操作） 123cd /export/server/ln -s jdk1.8.0_241/ jdk *三、Hadoop安装配置* \\1. 上传hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 到 &#x2F;export&#x2F;server 并解压文件 1tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz \\2. 修改配置文件 1cd /export/server/hadoop-3.3.0/etc/hadoop - hadoop-env.sh #文件最后添加 12345678910111213export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root - core-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;!-- 整合hive 用户代理设置 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!-- 文件系统垃圾桶保存时间 --&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; - hdfs-site.xml 123456789 &lt;!-- 设置SNN进程运行机器位置信息 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node2:9868&lt;/value&gt;&lt;/property&gt; - mapred-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 &lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- MR程序历史服务地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- MR程序历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt; - yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 &lt;!-- 设置YARN集群主角色运行机器位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否将对容器实施物理内存限制 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否将对容器实施虚拟内存限制。 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启日志聚集 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置yarn历史服务器地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史日志保存的时间 7天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; - workers 12345node1.itcast.cnnode2.itcast.cnnode3.itcast.cn \\3. 将node1的hadoop-3.3.0分发到node2、node3 12345cd /export/serverscp -r /export/server/hadoop-3.3.0 root@node2:$PWDscp -r /export/server/hadoop-3.3.0 root@node3:$PWD \\4. 将hadoop添加到环境变量 1vim /etc/profile 123export HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin \\5. 重新加载环境变量文件 1source /etc/profile \\6. Hadoop集群启动 （1）格式化namenode（只有首次启动需要格式化） 1hdfs namenode -format （2）脚本一键启动 \\7. WEB界面 （1）HDFS集群：http://node1:9870/ （2）YARN集群：http://node1:8088/ *四、zookeeper安装配置* \\1. 上传zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 123cd /export/server/tar -zxvf zookeeper-3.4.10.tar.gz \\2. 创建软连接 123cd /export/server/ln -s zookeeper-3.4.10/ zookeeper \\3. 修改配置文件 1cd /export/server/zookeeper/conf/ (1)将zoo_sample.cfg复制为zoo.cfg 1cp zoo_sample.cfg zoo.cfg 1vim zoo.cfg 将zoo.cfg修改为以下内容 12345678910111213141516171819#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas\\# 保留多少个快照autopurge.snapRetainCount=3\\# 日志多少小时清理一次autopurge.purgeInterval=1\\# 集群中服务器地址server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888 (2) 在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1 1echo 1 &gt; /export/server/zookeeper/zkdatas/myid (3) 将node1中&#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10分发给node2、node3 123scp -r /export/server/zookeeper-3.4.10/ slave1:$PWD scp -r /export/server/zookeeper-3.4.10/ slave2:$PWD (4) 在node2和node3上创建软连接 1ln -s zookeeper-3.4.10/ zookeeper (5) 分别在node2、node3上修改myid的值为2，3 12345cd /export/server/echo 2 &gt; /export/server/zookeeper/zkdatas/myidecho 3 &gt; /export/server/zookeeper/zkdatas/myid (6) 配置zookeeper的环境变量（三台都需要配置） 12345vim /etc/profileexport ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin (7) 重新加载环境变量 1source /etc/profile (8) 三台机器开启zookeeper 123cd /export/server/zookerper-3.4.10/binzkServer.sh start (9) 结果显示 (10) 查看zookeeper状态","categories":[],"tags":[]},{"title":"Spark HA & Yarn配置","slug":"three","date":"2022-05-22T03:11:00.000Z","updated":"2022-05-22T12:55:30.693Z","comments":true,"path":"2022/05/22/three/","link":"","permalink":"http://example.com/2022/05/22/three/","excerpt":"","text":"*七、Spark-Standalone-HA模式* 注：此处因为先前配置时的zookeeper版本和spark版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的zookeeper。配置之前需要删除三台主机的旧版zookeeper以及对应的软连接。 在node1节点上重新进行前面配置的zookerper操作 \\1. 上传apache-zookeeper-3.7.0-bin.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 123cd /export/server/tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz \\2. 在&#x2F;export&#x2F;server&#x2F;目录下创建软连接 123cd /export/server/ln -s apache-zookeeper-3.7.0-bin spark \\3. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;将zoo_sample.cfg文件复制为新文件 zoo.cfg \\4. 接上步给zoo.cfg 添加内容 \\5. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将1写入进去 \\6. 将node1节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容分发给node2和node3 \\7. 分发完后，分别在node2和node3上创建软连接 \\8. 将node2和node3的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的myid中的内容分别改为2和3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. 1cd /export/server/spark/conf 1vim spark-env.sh 删除: SPARK_MASTER_HOST&#x3D;node1 在文末添加内容 1234567891011SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER - Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha&quot; \\# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 \\# 指定Zookeeper的连接地址 \\# 指定在Zookeeper中注册临时节点的路径 \\9. 分发spark-env.sh到node2和node3上 123scp spark-env.sh node2:/export/server/spark/conf/ scp spark-env.sh node3:/export/server/spark/conf/ \\10. 启动之前确保 Zookeeper 和 HDFS 均已经启动 启动集群: # 在node1上 启动一个master 和全部worker 1sbin/start-all.sh # 注意, 下面命令在node2上执行 1sbin/start-master.sh # 在node2上启动一个备用的master进程 #将node1的master kill掉，查看node2的WebUI界面 *八、Spark-yarn模式* 1、启动yarn的历史服务器，jps看进程 2、在yarn上启动pyspark 3、命令测试 4、提交任务测试 5、client模式测试pi 6、cluster模式测试pi","categories":[],"tags":[]},{"title":"Spark local& stand-alone配置","slug":"two","date":"2022-05-22T03:11:00.000Z","updated":"2022-05-22T12:55:30.672Z","comments":true,"path":"2022/05/22/two/","link":"","permalink":"http://example.com/2022/05/22/two/","excerpt":"","text":"*五、Spark-local模式* \\1. 上传并安装Anaconda3-2021.05-Linux-x86_64.sh文件 123cd /export/server/sh Anaconda3-2021.05-Linux-x86_64.sh \\2. 过程显示： 12345678910111213...# 出现内容选 yes Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27; &gt;&gt;&gt; yes... # 出现添加路径：/export/server/anaconda3...[/root/anaconda3]&gt;&gt;&gt;/export/server/anaconda3 PREFIX=/export/server/anaconda3... \\3. 安装完成后，重新启动 看到base就表示安装完成了 \\4. 创建虚拟环境pyspark基于python3.8 1conda create -n pyspark python=3.8 \\5. 切换到虚拟环境内 1conda activate pyspark \\6. 在虚拟环境内安装包 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple \\7. 上传并解压spark-3.2.0-bin-hadoop3.2.tgz 123cd /export/servertar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ \\8. 创建软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark \\9. 添加环境变量 1vim /etc/profile SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 1vim .bashrc 内容添加进去： 1234567#JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python \\10. 重新加载环境变量 123source /etc/profilesource ~/.bashrc \\11. 开启spark 123cd /export/server/anaconda3/ens/pyspark/bin/./pyspark \\12. 进入WEB界面（node1:4040&#x2F;） \\13. 退出 1conda deactivate *六、Spark-Standalone模式* \\1. 在node2、node3上安装Python(Anaconda) 出现base表明安装完成 \\2. 将node1上的profile和.&#x2F;bashrc分发给node2、node3 #分发.bashrc 123scp ~/.bashrc root@node2:~/scp ~/.bashrc root@node3:~/ #分发profile 123scp /etc/profile/ root@node2:/etc/scp /etc/profile/ root@node3:/etc/ \\3. 创建虚拟环境pyspark基于python3.8 1conda create -n pyspark python=3.8 \\4. 切换到虚拟环境 1conda activate pyspark \\5. 在虚拟环境内安装包 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple \\6. 修改配置文件 1cd /export/server/spark/conf -配置workers 1mv workers.template workers 1vim workers # 将里面的localhost删除, 追加 12345node1 node2 node3 -配置spark-env.sh 1mv spark-env.sh.template spark-env.sh 1vim spark-env.sh 在底部追加如下内容 12345678910111213141516171819## 设置JAVA安装目录 JAVA_HOME=/export/server/jdk ## HADOOP软件配置文件目录,读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080 # worker cpu可用核数 SPARK_WORKER_CORES=1 # worker可用内存 SPARK_WORKER_MEMORY=1g # worker的工作通讯地址 SPARK_WORKER_PORT=7078# worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081 ## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=&quot;Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ Dspark.history.fs.cleaner.enabled=true&quot; \\7. 在HDFS上创建程序运行历史记录存放的文件夹: 123hadoop fs -mkdir /sparklog hadoop fs -chmod 777 /sparklog -配置spark-defaults.conf.template 1mv spark-defaults.conf.template spark-defaults.conf 1vim spark-defaults.conf # 修改内容, 追加如下内容 12345# 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://node1:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true -配置log4j.properties 1mv log4j.properties.template log4j.properties 1vim log4j.properties \\8. 将node1的spark分发到node2、node3 12345cd /export/server/scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:$PWDscp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:$PWD \\9. 在node2和node3上做软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark \\10. 重新加载环境变量 1source /etc/profile \\11. 启动历史服务器 123cd /export/server/spark/sbin./start-history-server.sh \\12. 访问WebUI界面（http://node1:18080/） \\13. 启动Spark的Master和Worker # 启动全部master和worker sbin&#x2F;start-all.sh # 或者可以一个个启动: # 启动当前机器的master 1sbin/start-master.sh # 启动当前机器的worker 1sbin/start-worker.sh # 停止全部 1sbin/stop-all.sh # 停止当前机器的master 1sbin/stop-master.sh # 停止当前机器的worker 1sbin/stop-worker.sh \\14. 访问WebUI界面（http://node1:8080/）","categories":[],"tags":[]}],"categories":[],"tags":[]}