{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"《Kafka命令行操作》","slug":"kafka2","date":"2022-06-07T13:20:41.000Z","updated":"2022-06-07T13:20:41.000Z","comments":true,"path":"2022/06/07/kafka2/","link":"","permalink":"http://example.com/2022/06/07/kafka2/","excerpt":"","text":"《Kafka命令行操作》 title: 《Kafka命令行操作》date: 2022-6-7 21:20:41description: Spark (HA) Spark (Yarn) 阅读全文 ** 1、cd &#x2F;export&#x2F;server&#x2F;kafka&#x2F;bin目录下的命令行工具 kafka-configs.sh 用于配置管理 kafka-console-consumer.sh 用于消费消息 kafka-console-producer.sh 用于生产消息 kafka-consumer-perf-test.sh 用于测试消费性能 kafka-topics.sh 用于管理主题 kafka-dump-log.sh 用于查看日志内容 kafka-server-stop.sh 用于关闭kafka服务 kafka-preferred-replica-election.sh 用于优先副本的选举 kafka-server-start.sh 用于启动kafka服务 kafka-producer-perf-test.sh 用于测试生产性能 kafka-reassign-partitions.sh 用于分区重分配 2、查看当前可用topic（应该什么也没有） 3、创建topic，检查是否创建成功 4、手动指定副本的存储位置，进入cd &#x2F;export&#x2F;data&#x2F;kafka-logs路径下查看分区的分布 5、进入到zookeeper client，查看目录、controller、controller_epoch 6、删除topic，检查是否删除成功 7、查看topic详情 8、增加分区数，查看topic详情 9、修改参数 10、生产者写入数据、消费者拉取数据**","categories":[],"tags":[]},{"title":"《Kafka环境配置》","slug":"kafak1","date":"2022-06-07T13:20:41.000Z","updated":"2022-06-07T13:20:41.000Z","comments":true,"path":"2022/06/07/kafak1/","link":"","permalink":"http://example.com/2022/06/07/kafak1/","excerpt":"","text":"《Kafka环境配置》 title: 《Kafka环境配置》date: 2022-6-7 21:20:41description: Spark (HA) Spark (Yarn) 阅读全文 **kafka 安装配置Kafka是一种高吞吐量的分布式发布订阅消息系统，其在大数据开发应用上的目的是通过 Hadoo的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。大数据开发需掌握Kafka架构原理及各组件的作用和使用方法及相关功能的实现。 上传文件包 到&#x2F;export&#x2F;server&#x2F;解压文件 tar -zxvf kafka_2.11-2.0.0.tgz 创建软连接 ln -s kafka_2.11-2.0.0/ kafka 进入 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config 修改 配置文件 server.properties cd &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config vim server.properties (1)21 行内容 broker.id&#x3D;0 为依次增长的:0、1、2、3、4,集群中唯一 id 从0开始，每台不能重复（注：此处不用修改） 21 broker.id&#x3D;0 (2)31 行内容 #listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;:9092 取消注释，内容改为：listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;master:9092PLAINTEXT为通信使用明文（加密ssl） 31 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;master:9092 (3)59 行内容 log.dirs&#x3D;&#x2F;tmp&#x2F;kafka-logs 为默认日志文件存储的位置，改为log.dirs&#x3D;&#x2F;export&#x2F;server&#x2F;data&#x2F;kafka-logs 59 log.dirs&#x3D;&#x2F;export&#x2F;data&#x2F;kafka-logs (4)63 行内容为 num.partitions&#x3D;1 是默认分区数 63 num.partitions=1 (5)76 行部分 ############################ Log Flush Policy ############################### 数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上）interval.messages interval.ms (6)93 行部分 ########################### **Log Retention Policy**############################ 数据保留策略 168/24=7，1073741824/1024=1GB，300000ms = 300s = 5min超过了删掉 （最后修改时间还是创建时间--&gt;日志段中最晚的一条消息，维护这个最大的时间戳--&gt;用户无法 干预 (7)121 行内容 zookeeper.connect&#x3D;localhost:2181 修改为zookeeper.connect&#x3D;master:2181,slave1:2181,slave2:2181 121 zookeeper.connect&#x3D;master:2181,slave1:2181,slave2:2181 (8)126 行内容 group.initial.rebalance.delay.ms&#x3D;0 修改为group.initial.rebalance.delay.ms&#x3D;3000 133 group.initial.rebalance.delay.ms&#x3D;3000 (9)给 slaves1和 slavs2 scp 分发 kafkacd &#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; slave2:$PWD (10)创建软连接 ln -s &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; kafka (11)配置 kafka 环境变量（注：可以一台一台配，也可以在 master 完成后发给 slave1 和slave2） vim &#x2F;etc&#x2F;profile # kafka 环境变量 export KAFKA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;kafka export PATH&#x3D;$PATH:$KAFKA_HOME&#x2F;bin (12)重新加载环境变量 source &#x2F;etc&#x2F;profile (13)分别在 slave1 和slave2 上修改配置文件 路径：&#x2F;export&#x2F;server&#x2F;kafka&#x2F;config将文件 server.properties 的第 21 行的 broker.id&#x3D;0 修改为 broker.id&#x3D;1 同理 slave2 同样操作 21 broker.id&#x3D;1 (14)将文件 server.properties 的第 31 行的 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;master:9092 修改为listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;slave1:9092 同理slave2 同样操作 31 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;slave1:9092 (15)启停 kafka (注：kafka 启动需要在 zookeeper 启动的情况下才可) kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties hadoop，zookeeper，kafka启动 结果显示： (base) [root@master ~]# jps 11793 NodeManager 91699 Kafka 85618 QuorumPeerMain 10697 NameNode 10924 DataNode 11596ResourceManager 109852 Jps [root@slave1 ~]# jps 9301 DataNode 9493 SecondaryNameNode 95959 Kafka 102971 Jps 9855 NodeManager 89534 QuorumPeerMain [root@slave2 ~]# jps 88660 QuorumPeerMain 95204 Kafka 9110 NodeManager 8616 DataNode 102104 Jps 关闭 kafka kafka-server-stop.sh stop 定制脚本一键启动 vim kafka-all.sh 放入 &#x2F;bin 路径下 #!&#x2F;bin&#x2F;bash if [ $# -eq 0 ] ; then echo “please input param:start stop” else if [ $1 &#x3D; start ] ;then echo “${1}ing master” ssh master “source &#x2F;etc&#x2F;profile;kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties” for i in {1..2} do echo “${1}ing slave${i}” ssh slave${i} “source &#x2F;etc&#x2F;profile;kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties” done fi if [ $1 &#x3D; stop ];then echo “${1}ping master “ ssh master “source &#x2F;etc&#x2F;profile;kafka-server-stop.sh” for i in {1..2} do echo “${1}ping slave${i}” ssh slave${i} “source &#x2F;etc&#x2F;profile;kafka-server-stop.sh” done fi fi**","categories":[],"tags":[]},{"title":"《kafka API使用方法》","slug":"kafka3","date":"2022-06-07T13:20:41.000Z","updated":"2022-06-07T13:20:41.000Z","comments":true,"path":"2022/06/07/kafka3/","link":"","permalink":"http://example.com/2022/06/07/kafka3/","excerpt":"","text":"《kafka API使用方法》 title: 《kafka API使用方法》date: 2022-6-7 21:20:41description: Spark (HA) Spark (Yarn) 阅读全文 1、一个正常的生产逻辑需要具备以下几个步骤 >>> （1）、配置生产者客户端参数及创建相应的生产者实例； （2）、构建待发送的消息； （3）、发送消息； （4）、关闭生产者实例。 2、生产者API采用默认分区方式将消息散列的发送到各个分区中。 3、什么时候会发生重复写入：producer的重试机制中，检测到一条数据的发送失败，而实际上已经发送成功，只是因为服务端响应超时。 4、什么时候会发生数据写入的丢失：ack参数的配置。 5、ack模式：取值0，1，all 0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是销量最高。 1代表producer往集群发送数据只要leader成功写入消息就可以发送下一条，只确保leader接收成功。 -1或all代表producer往集群发送数据需要所有的ISR Follow都完成从Leader的同步才会发送下一条，确保leader发送成功和所有的副本都成功接收。安全性最高，但是效率最低。 6、Properties props &#x3D; new Properties(); -&gt;配置生产者客户端参数 7、props.put(“bootstrap.servers”, “node1:9092,node2:9092,node3:9092”);-&gt;设置 kafka 集群的地址 8、props.put(“retries”, 3); -&gt;失败重试次数 9、props.put(“batch.size”, 10); -&gt;数据发送的批次大小 10、props.put(“linger.ms”, 10000); -&gt;消息在缓冲区保留的时间，超过设置的值就会被提交到服务端。 11、props.put(“max.request.size”,10); -&gt;数据发送请求的最大缓存数 12、props.put(“buffer.memory”, 10240); -&gt;整个 Producer 用到总内存的大小，如果缓冲区满了会提交数据到服务端&#x2F;&#x2F;buffer.memory 要大于 batch.size，否则会报申请内存不足的错误，降低阻塞的可能性。 13、props.put(“key.serializer”, “org.apache.kafka.common.serialization.StringSerializer”); -&gt;key-value序列化器 14、props.put(“value.serializer”, “org.apache.kafka.common.serialization.StringSerializer”); -&gt;字符串最好 15、在 Kafka 生产者客户端 KatkaProducer 中有3个参数是必填的 -&gt; bootstrap.servers、key.serializer、value.serializer 16、生产者api参数发送方式（发后即忘）： 发后即忘，它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下，这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。 ack-&gt;作用在broker Future send &#x3D; producer.send(rcd);-&gt;也是异步 17、生产者api参数发送方式（同步发送）： producer.send(rcd).get(); -&gt;一旦调用get方法，就会阻塞 Future future = Callable.run()-&gt;有返回值，future.get() Runnable.run()-&gt;无返回值 18、生产者api参数发送方式（异步发送）： 回调函数会在 producer 收到ack时调用，为异步调用，该方法有两个参数，分别是 RecordMetadata 和Exception，如果 Exception 为 null，说明消息发送成功，如果Exception不为 null，说明消息发送失败。 注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。 19、幂等性：生产者将一条数据多次重复写入的情况下，broker端依然只有一条。 20、在IJ中新建Maven项目，配置pom.xml，重新加载Maven项目 21、新建ProducerDemo类、ProducerCallbackDemo类 22、生产者原理 （1）、一个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程 。 （2）、在主线程中由 kafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器(RecordAccumulator，也称为消息收集器)中。 （3）、Sender 线程负责从 RecordAccumulator 获取消息并将其发送到 Kafka 中； （4）、RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。 （5）、RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 配置，默认值为 33554432B，即 32M。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候 KafkaProducer.send()方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms 的配置，此参数的默认值为 60000，即 60 秒。 （6）、主线程中发送过来的消息都会被迫加到 RecordAccumulator 的某个双端队列( Deque )中， RecordAccumulator 内部为每个分区都维护了一个双端队列，即 Deque。消息写入缓存时,追加到双端队列的尾部。 （7）、Sender 读取消息时，从双端队列的头部读取。 （8）、注意：ProducerBatch 是指一个消息批次；与此同时，会将较小的 ProducerBatch 凑成一个较大 ProducerBatch，也可以减少网络请求的次数以提升整体的吞吐量。 （9）、当topic中的分区数增多的情况下，recordaccumulator中的分区数就会增多。当topic的数量增多的情况下，recordaccumulator中的分区数也会增多。 （10）、ProducerBatch 大小和 batch.size 参数也有着密切的关系。 （11）、当一条消息(ProducerRecord) 流入RecordAccumulator时，会先寻找与消息分区所对应的双端队列(如果没有则新建)，再从这个双端队列的尾部获取一个 ProducerBatch (如果没有则新建)，查看 ProducerBatch 中是否还可以写入这个 ProducerRecord，如果可以写入，如果不可以则需要创建一个新的 Producer Batch。 （12）、在新建ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数大小，如果不超过，那么就以 batch.size 参数的大小来创建 ProducerBatch。如果生产者客户端需要向很多分区发送消息，则可以将 buffer.memory 参数适当调大以增加整体的吞吐量。 （13）、Sender 从 RecordAccumulator 获取缓存的消息之后，会进一步将&lt;分区，Deque&gt;的形式转变成&lt;Node,List&lt; ProducerBatch&gt;的形式，其中 Node 表示 Kafka 集群 broker 节点。 （14）、对于网络连接来说，生产者客户端是与具体 broker 节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区。 （15）、而对于 KafkaProducer 的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络 I&#x2F;O 层面的转换。 （16）、在转换成&lt;Node, List&gt;的形式之后，Sender 会进一步封装成&lt;Node,Request&gt; 的形式，这样就可以将 Request 请求发往各个 Node了，这里的 Request 是 Kafka 各种协议请求。 （17）、请求在从sender线程发往 Kafka 之前还会保存到InFlightRequests中，InFlightRequests 保存对象的具体形式为 Map&lt;Nodeld, Deque&gt;，它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 id 编号)。 （18）、与此同时，InFlightRequests 还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接(也就是客户端与 Node 之间的连接) 最多缓存的请求数。 （19）、这个配置参数为max.in.flight.request.per.Connection，默认值为5，即每个连接最多只能缓存5个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应( Response )。 （20）、通过比较 Deque 的 size 与这个参数的大小来判断对应的 Node 中是否己经堆积了很多未响应的消息，如果真是如此，那么说明这个 Node 节点负载较大或网络连接有问题，再继其发送请求会增大请求超时的可能。 23、重要的生产者参数 （1）、max.request.size这个参数用来限制生产者客户端能发送的消息的最大值，默认值为1048576B，即 1MB 一般情况下，这个默认值就可以满足大多数的应用场景了。这个参数还涉及一些其它参数的联动，比如 broker 端的 message.max.bytes 参数，如果配置错误可能会引起一些不必要的异常；比如将broker 端的message.max.bytes 参数配置为 10，而max.request.size 参数配置为20，那么当发送一条大小为15B的消息时，生产者客户端就会报出异常。 （2）、compression.type这个参数用来指定消息的压缩方式，默认值为“none”，即默认情况下，消息不会被压缩。该参数还可以配置为”gzip”，”snappy” 和 “lz4”。（服务端也有压缩参数，先解压，再压缩）；对消息进行压缩可以极大地减少网络传输、降低网络 I&#x2F;O，从而提高整体的性能。消息压缩是一种以时间换空间的优化方式，如果对时延有一定的要求，则不推荐对消息进行压缩；没有必要，不需要压缩。 （3）、retries 和 retry.backoff.msretries 参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作。消息在从生产者发出到成功写入服务器之前可能发生一些临时性的异常，比如网络抖动、 leader 副本的选举等，这种异常往往是可以自行恢复的，生产者可以通过配置 retries 大于 0 的值，以此通过内部重试来恢复而不是一味地将异常抛给生产者的应用程序。如果重试达到设定的次数，那么生产者就会放弃重试并返回异常。重试还和另一个参数retry.backoff.ms有关，这个参数的默认值为100，它用来设定两次重试之间的时间间隔，避免无效的频繁重试。Kafka 可以保证同一个分区中的消息是有序的。如果生产者按照一定的顺序发送消息，那么这些消息也会顺序地写入分区，进而消费者也可以按照同样的顺序消费它们。对于某些应用来说，顺序性非常重要，比如 MySQL binlog 的传输，如果出现错误就会造成非常严重的后果；MySQL binlog–&gt;mysql插入数据–&gt;操作结果体会在表中–&gt;mysql为了提高可靠性会把操作记录在日志中–&gt;为了以后的主从同步（mysql集群，主表，子表）–&gt;读写分离–&gt;binlog（mysql自己设计的格式，二进制形式）。如果将 acks 参数配置为非零值，并且max.flight.requests.per.connection 参数配置为大于1的值，那可能会出现错序的现象：如果第一批次消息写入失败，而第二批次消息写入成功，那么生产者会重试发送第一批次的消息，此时如果第一次的消息写入成功，那么这两个批次的消息就出现了错序。一般而言，在需要保证消息顺序的场合建议把参数max.in.flight.requests.per.connection 配置为 1 ，而不是把 acks 配置为0，不过这样也会影响整体的吞吐。–&gt;吞吐量降低 （4）、batch.size每个Batch 要存放 batch.size 大小的数据后，才可以发送出去。比如说 batch.size 默认值是 16KB，那么里面凑够 16KB 的数据才会发送。理论上来说，提升 batch.size 的大小，可以允许更多的数据缓冲在里面，那么一次 Request 发送出去的数据量就更多了，这样吞吐量可能会有所提升。但是 batch.size 也不能过大，要是数据老是缓冲在 Batch 里迟迟不发送出去，那么发送消息的延迟就会很高。一般可以尝试把这个参数调节大些，利用生产环境发消息负载测试一下。 （5）、linger.ms（和batchsize有联系）这个参数用来指定生产者发送 ProducerBatch 之前等待更多消息( ProducerRecord )加入ProducerBatch 时间，默认值为0。生产者客户端会在ProducerBatch 填满或等待时间超过 linger.ms 值时发送出去。增大这个参数的值会增加消息的延迟，但是同时能提升一定的吞吐量。 （6）、enable.idempotence是否开启幂等性功能,详见后续原理加强；幂等性,就是一个操作重复做，每次的结果都一样，x1&#x3D;1，x1&#x3D;1，x*1&#x3D;1；在 kafka 中，就是生产者生产的一条消息，如果多次重复发送，在服务器中的结果还是只有一条。kafka很难实现幂等性，如果重复发，kafka肯定有多条消息–&gt;需要有机制判断曾经是否发送过–&gt;各种手段判断–&gt;事务管理的概念–&gt;加入幂等性，吞吐量会急剧下降。 （7）、partitioner.classes用来指定分区器，默认：org.apache.kafka.internals.DefaultPartitioner –&gt;用hashcode分自定义 partitioner 需要实现 org.apache.kafka.clients.producer.Partitioner 接口 –&gt;可以通过partitioner接口自己实现分区器 四、消费者API1、一个正常的消费逻辑需要具备以下几个步骤： （1）、配置消费者客户端参数 （2）、创建相应的消费者实例 （3）、订阅主题 （4）、拉取消息并消费 （5）、提交消费位移 offset （6）、关闭消费者实例 2、subscribe重载方法： （1）、前面两种通过集合的方式订阅一到多个topic public void subscribe(Collection topics,ConsumerRebalanceListener listener)public void subscribe(Collection topics) （2）、后两种主要是采用正则的方式订阅一到多个topicpublic void subscribe(Pattern pattern, ConsumerRebalanceListener listener)public void subscribe(Pattern pattern) （3）、正则方式订阅主题（只要是tpc_数字的形式，三位数字以内）如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中，如果有人又创建了新的主题，并且主题名字与正表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并且可以处理不同的类型，那么这种订阅方式就很有效。利用正则表达式订阅主题，可实现动态订阅； 3、assign订阅主题 消费者不仅可以通过 KafkaConsumer.subscribe()方法订阅主题，还可直接订阅某些主题的指定分区； 在KafkaConsumer 中提供了assign()方法来实现这些功能，此方法的具体定义如下：public void assign(Collection&lt;TopicPartition&gt; partitions) ； 这个方法只接受参数 partitions，用来指定需要订阅的分区集合。 示例如下: consumer.assign(Arrays.asList(new TopicPartition (&quot;tpc_1&quot;,0),new TopicPartition(“tpc_2”,1)))； 4、subscribe与assign的区别 （1）、通过 subscribe()方法订阅主题具有消费者自动再均衡功能； 在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。 （2）、assign() 方法订阅分区时，是不具备消费者自动均衡的功能的； 其实这一点从 assign()方法参数可以看出端倪,两种类型subscribe()都有ConsumerRebalanceListener类型参数的方法，而assign()方法却没有。 5、取消订阅 （1）、可以使用KafkaConsumer中的unsubscribe()方法采取消主题的订阅，这个方法既可以取消通过subscribe( Collection)方式实现的订阅; （2）、也可以取消通过subscribe(Pattem)方式实现的订阅，还可以取消通过 assign(Collection)方式实现的订阅。 （3）、如果将subscribe(Collection)或 assign(Collection)集合参数设置为空集合，作用与 unsubscribe()方法相同，如下示例中三行代码的效果相同： consumer.unsubscribe(); consumer.subscribe(new ArrayList&lt;String&gt;()) ; consumer.assign(new ArrayList&lt;TopicPartition&gt;()); 6、消息的消费模式 Kafka中的消费是基于拉取模式的。消息的消费一般有两种模式：推送模式和拉取模式。 推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。 Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用 poll()方法，poll()方法返回的是所订阅的主题(分区)上的一组消息。对于poll () 方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息，那么 poll()方法返回为空的消息集; poll ()方法具体定义如下: public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout) 超时时间参数 timeout，用来控制poll()方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率，可以把timeout设置为Long.MAX_VALUE; 消费者消费到的每条消息的类型为 ConsumerRecord topic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号 offset 表示消息在所属分区的偏移量 timestamp 表示时间戳，与此对应的timestampType表示时间戳的类型 timestampType 有两种类型CreateTime和LogAppendTime，分别代表消息创建的时间戳和消息追加到日志的时间戳 headers 表示消息的头部内容 key value 分别表示消息的键和消息的值,一般业务应用要读取的就是value serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小，如果 key 为空, 则 serializedKeySize 值为-1，同样，如果value 为空，则serializedValueSize的值也会为-1 checksum 是 CRC32 的校验值 7、指定位移消费 有些时候，我们需要一种更细粒度的掌控，可以让我们从特定的位移处开始拉取消息，而KafkaConsumer中的seek()方法正好提供了这个功能，让我们可以追前消费或回溯消费。seek()方法的具体定义如下: public void seek(TopicPartiton partition,long offset) 8、再均衡监听器 一个消费组中，一旦有消费者的增减发生，会触发消费者组的 rebalance 再均衡；如果 A 消费者消费掉的一批消息还没来得及提交 offset，而它所负责的分区在 rebalance 中转移给了B消费者，则有可能发生数据的重复消费处理。此情形下，可以通过再均衡监听器做一定程度的补救； 9、自动位移提交 Kafka中默认的消费位移的提交方式是自动提交，这个由消费者客户端参数enable.auto.commit 配置，默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置，默认值为 5 秒，此参数生效的前提是 enable.auto.commit 参数为 true。 在默认的方式下，消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在poll()方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。 Kafka消费的编程逻辑中位移提交是一大难点，自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。 重复消费 &gt;&gt;&gt; 假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。 丢失消息 &gt;&gt;&gt; 按照一般思维逻辑而言，自动提交是延时提交，重复消费可以理解，那么消息丢失又是在什么情形下会发生的呢？我们来看下图中的情形：拉取线程不断地拉取消息并存入本地缓存，比如在 BlockingQueue中，另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取，以及第 m 次位移提交的时候，也就是x+6 之前的位移己经确认提交了，处理线程却还正在处理 x+3 的消息；此时如果处理线程发生了异常，待其恢复之后会从第 m 次位移提交处，也就是 x+6 的位置开始拉取消息，那么 x+3 至 x+6 之间的消息就没有得到相应的处理，这样便发生消息丢失的现象。 10、手动位移提交 自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象，但是在编程的世界里异常无可避免；同时，自动位移提交也无法做到精确的位移管理。在 Kafka 中还提供了手动位移提交的方式，这样可以使得开发人员对消费位移的管理控制更加灵活。 很多时候并不是说拉取到消息就算消费完成，而是需要将消息写入数据库、写入本地缓存，或者是更加复杂的业务处理。在这些场景下，所有的业务处理完成才能认为消息被成功消费； 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 fals，示例如下 props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, false); 手动提交可以细分为同步提交和异步提交，对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。 同步提交的方式：对于采用 commitSync()的无参方法，它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的，如果想寻求更细粒度的、更精准的提交，那么就需要使用 commitSync()的另一个有参方法，具体定义如下： public void commitSync(final Map&lt;TopicPartition,OffsetAndMetadata&gt; offsets) 异步提交方式：commitSync()方法相反，异步提交的方式( commitAsync())在执行的时候消费者线程不会被阻塞；可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操。异步提交以便消费者的性能得到一定的增强。 11、其他重要参数 fetch.min.bytes=1B -&gt; 一次拉取的最小字节数 fetch.max.bytes=50M -&gt; 一次拉取的最大数据量 fetch.max.wait.ms=500ms -&gt; 拉取时的最大等待时长 max.partition.fetch.bytes = 1MB -&gt; 每个分区一次拉取的最大数据量 max.poll.records=500-&gt; 一次拉取的最大条数 connections.max.idle.ms=540000ms -&gt; 网络连接的最大闲置时长 request.timeout.ms=30000ms -&gt; 一次请求等待响应的最大超时时间consumer 等待请求响应的最长时间 metadata.max.age.ms=300000 -&gt; 元数据在限定时间内没有进行更新,则会被强制更新 reconnect.backoff.ms=50ms -&gt; 尝试重新连接指定主机之前的退避时间 retry.backoff.ms=100ms -&gt; 尝试重新拉取数据的重试间隔 12、新建ConsumerDemo、ConsumerDemo1、ConsumerTask、ConsumerDemo2、ConsumerSeekOffset类 五、Topic管理API1、一般情况下，我们都习惯使用 kafka-topic.sh 本来管理主题，如果希望将管理类的功能集成到公司内部的系统中，打造集管理、监控、运维、告警为一体的生态平台，那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类。 KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法： 创建主题：CreateTopicResult createTopics(Collection new Topics) 删除主题：DeleteTopicsResult deleteTopics(Collectiontopics) 列出所有可用的主题：ListTopicsResult listTopics() 查看主题的信息：DescribeTopicsResult describeTopics(Collection topicNames) 查询配置信息： DescribeConfigsResult describeConfigs(Collectionresources) 修改配置信息：AlterConfigsResult alterConfigs(Map&lt;ConfigResource,Config&gt; configs) 增加分区：CreatePartitionsResult createPartitions(Map&lt;String,NewPartitions&gt; new Partitions) 构造一个 KafkaAdminClient AdminClient adminClient &#x3D; KafkaAdminClient.create(props); 2、列出主题 ListTopicsResult listTopicsResult &#x3D; adminClient.listTopics(); Set topics &#x3D; listTopicsResult.names().get(); System.out.println(topics); 3、查看主题信息 DescribeTopicsResult describeTopicsResult &#x3D; adminClient.describeTopics(Arrays.asList(“tpc_4”, “tpc_3”)); Map&lt;String, TopicDescription&gt; res &#x3D; describeTopicsResult.all().get(); Set ksets &#x3D; res.keySet(); for (String k : ksets) { System.out.println(res.get(k)); } 4、创建主题 &#x2F;&#x2F; 参数配置 Properties props &#x3D; new Properties(); props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092,node3:9092”); props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,3000); &#x2F;&#x2F; 创建 admin client 对象 AdminClient adminClient &#x3D; KafkaAdminClient.create(props); &#x2F;&#x2F; 由服务端 controller 自行分配分区及副本所在 broker NewTopic tpc_3 &#x3D; new NewTopic(“tpc_3”, 2, (short) 1); &#x2F;&#x2F; 手动指定分区及副本的 broker 分配 HashMap&lt;Integer, List&gt; replicaAssignments &#x3D; new HashMap&lt;&gt;(); &#x2F;&#x2F; 分区 0,分配到 broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); &#x2F;&#x2F; 分区1,分配到 broker0,broker2 replicaAssignments.put(0,Arrays.asList(0,1)); NewTopic tpc_4 &#x3D; new NewTopic(“tpc_4”, replicaAssignments); CreateTopicsResult result &#x3D; adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); &#x2F;&#x2F; 从 future 中等待服务端返回 try { result.all().get(); } catch (Exception e) { e.printStackTrace(); } adminClient.close(); 5、删除主题 DeleteTopicsResult deleteTopicsResult &#x3D; adminClient.deleteTopics(Arrays.asList(“tpc_1”, “tpc_1”)); Map&lt;String, KafkaFuture&gt; values &#x3D; deleteTopicsResult.values(); System.out.println(values); 6、其他管理 除了进行 topic 管理之外，KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作； 7、新建KafkaAdminDemo、CallableDemo类** **","categories":[],"tags":[]},{"title":"Spark HA & Yarn配置","slug":"three","date":"2022-05-22T03:11:00.000Z","updated":"2022-05-22T12:55:30.693Z","comments":true,"path":"2022/05/22/three/","link":"","permalink":"http://example.com/2022/05/22/three/","excerpt":"","text":"*七、Spark-Standalone-HA模式* 注：此处因为先前配置时的zookeeper版本和spark版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的zookeeper。配置之前需要删除三台主机的旧版zookeeper以及对应的软连接。 在node1节点上重新进行前面配置的zookerper操作 \\1. 上传apache-zookeeper-3.7.0-bin.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 123cd /export/server/tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz \\2. 在&#x2F;export&#x2F;server&#x2F;目录下创建软连接 123cd /export/server/ln -s apache-zookeeper-3.7.0-bin spark \\3. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;将zoo_sample.cfg文件复制为新文件 zoo.cfg \\4. 接上步给zoo.cfg 添加内容 \\5. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将1写入进去 \\6. 将node1节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容分发给node2和node3 \\7. 分发完后，分别在node2和node3上创建软连接 \\8. 将node2和node3的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的myid中的内容分别改为2和3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. 1cd /export/server/spark/conf 1vim spark-env.sh 删除: SPARK_MASTER_HOST&#x3D;node1 在文末添加内容 1234567891011SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER - Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha&quot; \\# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 \\# 指定Zookeeper的连接地址 \\# 指定在Zookeeper中注册临时节点的路径 \\9. 分发spark-env.sh到node2和node3上 123scp spark-env.sh node2:/export/server/spark/conf/ scp spark-env.sh node3:/export/server/spark/conf/ \\10. 启动之前确保 Zookeeper 和 HDFS 均已经启动 启动集群: # 在node1上 启动一个master 和全部worker 1sbin/start-all.sh # 注意, 下面命令在node2上执行 1sbin/start-master.sh # 在node2上启动一个备用的master进程 #将node1的master kill掉，查看node2的WebUI界面 *八、Spark-yarn模式* 1、启动yarn的历史服务器，jps看进程 2、在yarn上启动pyspark 3、命令测试 4、提交任务测试 5、client模式测试pi 6、cluster模式测试pi","categories":[],"tags":[]},{"title":"Spark基础配置","slug":"one","date":"2022-05-22T03:11:00.000Z","updated":"2022-05-22T12:55:30.689Z","comments":true,"path":"2022/05/22/one/","link":"","permalink":"http://example.com/2022/05/22/one/","excerpt":"","text":"*一、配置基础环境* #主机名 1cat /etc/hostname # hosts映射 1vim /etc/hosts 1234567891011127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.151 node1.itcast.cn node1 192.168.88.152 node2.itcast.cn node2 192.168.88.153 node3.itcast.cn node3 *二、安装配置jdk* \\1. 编译环境软件安装目录 1mkdir -p /export/server \\2. 上传jdk-8u65-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录并解压 123rztar -zxvf jdk-8u65-linux-x64.tar.gz \\3. 配置环境变量 1vim /etc/profile 12345export JAVA_HOME=/export/server/jdk1.8.0_241export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar \\4. 重新加载环境变量文件 1source /etc/profile \\5. 查看java版本号 1Java -version \\6. 将java由node1分发到node2、node3 123scp -r /export/server/jdk1.8.0_241/ root@node2:/export/serverscp -r /export/server/jdk1.8.0_241/ root@node3:/export/server \\7. 配置node2、node3的环境变量文件（方法如上） \\8. 在node1、node2、node3中创建软连接（三台都需要操作） 123cd /export/server/ln -s jdk1.8.0_241/ jdk *三、Hadoop安装配置* \\1. 上传hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 到 &#x2F;export&#x2F;server 并解压文件 1tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz \\2. 修改配置文件 1cd /export/server/hadoop-3.3.0/etc/hadoop - hadoop-env.sh #文件最后添加 12345678910111213export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root - core-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;!-- 整合hive 用户代理设置 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!-- 文件系统垃圾桶保存时间 --&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; - hdfs-site.xml 123456789 &lt;!-- 设置SNN进程运行机器位置信息 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node2:9868&lt;/value&gt;&lt;/property&gt; - mapred-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 &lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- MR程序历史服务地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- MR程序历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt; - yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 &lt;!-- 设置YARN集群主角色运行机器位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否将对容器实施物理内存限制 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否将对容器实施虚拟内存限制。 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启日志聚集 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置yarn历史服务器地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史日志保存的时间 7天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; - workers 12345node1.itcast.cnnode2.itcast.cnnode3.itcast.cn \\3. 将node1的hadoop-3.3.0分发到node2、node3 12345cd /export/serverscp -r /export/server/hadoop-3.3.0 root@node2:$PWDscp -r /export/server/hadoop-3.3.0 root@node3:$PWD \\4. 将hadoop添加到环境变量 1vim /etc/profile 123export HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin \\5. 重新加载环境变量文件 1source /etc/profile \\6. Hadoop集群启动 （1）格式化namenode（只有首次启动需要格式化） 1hdfs namenode -format （2）脚本一键启动 \\7. WEB界面 （1）HDFS集群：http://node1:9870/ （2）YARN集群：http://node1:8088/ *四、zookeeper安装配置* \\1. 上传zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 123cd /export/server/tar -zxvf zookeeper-3.4.10.tar.gz \\2. 创建软连接 123cd /export/server/ln -s zookeeper-3.4.10/ zookeeper \\3. 修改配置文件 1cd /export/server/zookeeper/conf/ (1)将zoo_sample.cfg复制为zoo.cfg 1cp zoo_sample.cfg zoo.cfg 1vim zoo.cfg 将zoo.cfg修改为以下内容 12345678910111213141516171819#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas\\# 保留多少个快照autopurge.snapRetainCount=3\\# 日志多少小时清理一次autopurge.purgeInterval=1\\# 集群中服务器地址server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888 (2) 在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1 1echo 1 &gt; /export/server/zookeeper/zkdatas/myid (3) 将node1中&#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10分发给node2、node3 123scp -r /export/server/zookeeper-3.4.10/ slave1:$PWD scp -r /export/server/zookeeper-3.4.10/ slave2:$PWD (4) 在node2和node3上创建软连接 1ln -s zookeeper-3.4.10/ zookeeper (5) 分别在node2、node3上修改myid的值为2，3 12345cd /export/server/echo 2 &gt; /export/server/zookeeper/zkdatas/myidecho 3 &gt; /export/server/zookeeper/zkdatas/myid (6) 配置zookeeper的环境变量（三台都需要配置） 12345vim /etc/profileexport ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin (7) 重新加载环境变量 1source /etc/profile (8) 三台机器开启zookeeper 123cd /export/server/zookerper-3.4.10/binzkServer.sh start (9) 结果显示 (10) 查看zookeeper状态","categories":[],"tags":[]},{"title":"Spark local& stand-alone配置","slug":"two","date":"2022-05-22T03:11:00.000Z","updated":"2022-05-22T12:55:30.672Z","comments":true,"path":"2022/05/22/two/","link":"","permalink":"http://example.com/2022/05/22/two/","excerpt":"","text":"*五、Spark-local模式* \\1. 上传并安装Anaconda3-2021.05-Linux-x86_64.sh文件 123cd /export/server/sh Anaconda3-2021.05-Linux-x86_64.sh \\2. 过程显示： 12345678910111213...# 出现内容选 yes Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27; &gt;&gt;&gt; yes... # 出现添加路径：/export/server/anaconda3...[/root/anaconda3]&gt;&gt;&gt;/export/server/anaconda3 PREFIX=/export/server/anaconda3... \\3. 安装完成后，重新启动 看到base就表示安装完成了 \\4. 创建虚拟环境pyspark基于python3.8 1conda create -n pyspark python=3.8 \\5. 切换到虚拟环境内 1conda activate pyspark \\6. 在虚拟环境内安装包 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple \\7. 上传并解压spark-3.2.0-bin-hadoop3.2.tgz 123cd /export/servertar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ \\8. 创建软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark \\9. 添加环境变量 1vim /etc/profile SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 1vim .bashrc 内容添加进去： 1234567#JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python \\10. 重新加载环境变量 123source /etc/profilesource ~/.bashrc \\11. 开启spark 123cd /export/server/anaconda3/ens/pyspark/bin/./pyspark \\12. 进入WEB界面（node1:4040&#x2F;） \\13. 退出 1conda deactivate *六、Spark-Standalone模式* \\1. 在node2、node3上安装Python(Anaconda) 出现base表明安装完成 \\2. 将node1上的profile和.&#x2F;bashrc分发给node2、node3 #分发.bashrc 123scp ~/.bashrc root@node2:~/scp ~/.bashrc root@node3:~/ #分发profile 123scp /etc/profile/ root@node2:/etc/scp /etc/profile/ root@node3:/etc/ \\3. 创建虚拟环境pyspark基于python3.8 1conda create -n pyspark python=3.8 \\4. 切换到虚拟环境 1conda activate pyspark \\5. 在虚拟环境内安装包 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple \\6. 修改配置文件 1cd /export/server/spark/conf -配置workers 1mv workers.template workers 1vim workers # 将里面的localhost删除, 追加 12345node1 node2 node3 -配置spark-env.sh 1mv spark-env.sh.template spark-env.sh 1vim spark-env.sh 在底部追加如下内容 12345678910111213141516171819## 设置JAVA安装目录 JAVA_HOME=/export/server/jdk ## HADOOP软件配置文件目录,读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080 # worker cpu可用核数 SPARK_WORKER_CORES=1 # worker可用内存 SPARK_WORKER_MEMORY=1g # worker的工作通讯地址 SPARK_WORKER_PORT=7078# worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081 ## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=&quot;Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ Dspark.history.fs.cleaner.enabled=true&quot; \\7. 在HDFS上创建程序运行历史记录存放的文件夹: 123hadoop fs -mkdir /sparklog hadoop fs -chmod 777 /sparklog -配置spark-defaults.conf.template 1mv spark-defaults.conf.template spark-defaults.conf 1vim spark-defaults.conf # 修改内容, 追加如下内容 12345# 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://node1:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true -配置log4j.properties 1mv log4j.properties.template log4j.properties 1vim log4j.properties \\8. 将node1的spark分发到node2、node3 12345cd /export/server/scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:$PWDscp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:$PWD \\9. 在node2和node3上做软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark \\10. 重新加载环境变量 1source /etc/profile \\11. 启动历史服务器 123cd /export/server/spark/sbin./start-history-server.sh \\12. 访问WebUI界面（http://node1:18080/） \\13. 启动Spark的Master和Worker # 启动全部master和worker sbin&#x2F;start-all.sh # 或者可以一个个启动: # 启动当前机器的master 1sbin/start-master.sh # 启动当前机器的worker 1sbin/start-worker.sh # 停止全部 1sbin/stop-all.sh # 停止当前机器的master 1sbin/stop-master.sh # 停止当前机器的worker 1sbin/stop-worker.sh \\14. 访问WebUI界面（http://node1:8080/）","categories":[],"tags":[]}],"categories":[],"tags":[]}